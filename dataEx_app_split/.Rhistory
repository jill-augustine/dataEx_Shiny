install.packages("sparklyr")
library(sparklyr)
spark_available_versions()
spark_install()
sc <- spark_connect()
sc <- spark_connect(master = "local")
getwd()
link <- spark_read_parquet(sc,
link <- spark_read_parquet(sc,
name = "df",
path = patse0("C:/Users/u4xk7mg/Documents/HXWHDFSdata/customerbasedbilling/mpcdata/","gamer1")
link <- spark_read_parquet(sc,
name = "df",
path = paste0("C:/Users/u4xk7mg/Documents/HXWHDFSdata/customerbasedbilling/mpcdata/","gamer1"))
link <- spark_read_parquet(sc,
name = "df",
path = paste0("/Users/u4xk7mg/Documents/HXWHDFSdata/customerbasedbilling/mpcdata/","gamer1"))
install.packages("tidyverse")
library(readr)
dataset <- "gamer1"
csv <- read_csv(paste0("C:/Users/u4xk7mg/Documents/HXWHDFSdata/customerbasedbilling/mpcdata/",dataset))
dataset <- "gamer1"
csv <- read_csv(paste0("C:/Users/u4xk7mg/Documents/HXWHDFSdata/customerbasedbilling/mpcdata/",dataset,".csv"))
csv <- read_csv(paste0("HXWHDFSdata/customerbasedbilling/mpcdata/",dataset,".csv"))
paste0("HXWHDFSdata/customerbasedbilling/mpcdata/",dataset,".csv")
"HWXHDFSdata/customerbasedbilling/mpcdata/gamer1.csv"
gamer1 <- read_csv("HWXHDFSdata/customerbasedbilling/mpcdata/gamer1.csv")
csv <- read_csv(paste0("HXWHDFSdata/customerbasedbilling/mpcdata/",dataset,".csv"))
csv <- read_csv(file = paste0("HXWHDFSdata/customerbasedbilling/mpcdata/",dataset,".csv"))
csv <- read_csv(file = paste0("HWXHDFSdata/customerbasedbilling/mpcdata/",dataset,".csv"))
locale()
locale()[1]
locale()[2]
for (i in 1:20) {
print(locale()[i])
}
locale()[1]
locale()[[1]]
locale()[1][1]
locale()[1] %>% class()
locale()[[1]]] %>% class()
locale()[[1]] %>% class()
locale()[[1]] %>% head()
locale()[[1]]
for (i in 20) {
locale()[[1]]
}
for (i in 20) {
print(locale()[[1]])
}
print(locale()[[1]][i])
locale()[1]$names
locale()[1]$mon
coltypes <- "iiiiiccccccccccicccccccciiiicccccciiciciDDcciiii"
> for (i in 1:20) {
print(locale()[[1]])
}
for (i in 1:20) {
print(locale()[[1]])
}
link <- copy_to(sc, csv, "df")
link
View(csv)
colnames <- as.character(1:18)
colnames <- as.character(1:48)
csv <- read_csv(paste0("HWXHDFSdata/customerbasedbilling/mpcdata/",dataset,".csv"),
header ,
col_names = colnames,
col_types = coltypes)
View(csv)
csv <- read_csv(paste0("HWXHDFSdata/customerbasedbilling/mpcdata/",dataset,".csv"),
col_names = colnames,
col_types = coltypes)
coltypes <- "iiiiiccccccccccicccccccciiiicccccciiciciTTcciiii"
csv <- read_csv(paste0("HWXHDFSdata/customerbasedbilling/mpcdata/",dataset,".csv"),
col_names = colnames,
col_types = coltypes)
View(gamer1)
View(csv)
View(gamer1)
View(csv)
spark_disconnect(sc)
shiny::runApp('cbb_app_v5')
runApp('cbb_app_v5')
system(command = "pwd")
system(inter = TRUE, command = "pwd")
system(inter = TRUE, command = "pwd()")
system2("pwd")
shiny::runApp('cbb_app')
runApp('cbb_app')
runApp('cbb_app')
shiny::runApp('cbb_app')
read_csv("gamer1.csv")
shell('dir')
library(explore)
dwh <- dwh_connect("DWH1")
input_pwd <- encrypt(getPass::getPass("password:"))
dwh <- dwh_connect("DWH1", user="u4xk7mg", pwd=decrypt(input_pwd))
dwh <- dwh_connect("DWH64", user="u4xk7mg", pwd=decrypt(input_pwd))
dwh <- dwh_connect("DWH1", user="u4xk7mg", pwd=decrypt(input_pwd))
dwh <- dwh_connect("DWH1", user="u4xk7mg", pwd=decrypt(input_pwd))
library(explore)
input_pwd <- encrypt(getPass::getPass("password:"))
dwh <- dwh_connect("DWH1", user="u4xk7mg", pwd=decrypt(input_pwd))
library(explore)
input_pwd <- encrypt(getPass::getPass("password:"))
dwh <- dwh_connect("DWH1", user="u4xk7mg", pwd=decrypt(input_pwd))
library(RODBC)
conn <- odbcConnect(dsn= 'DWH1')
data <- sqlQuery(conn, "SELECT Top 1 * from DB_FB_CA.TechIndex_NPS_Daten")
data
close(conn)
conn <- odbcDriverConnect("Driver=Teradata;DBCName=DWH1;Uid=u4xk7mg;Pwd=C!lantr3")
explore::dwh_connect()
library(RODBC)
conn <- odbcDriverConnect("Driver=Teradata;DBCName=DWH1;Uid=u4xk7mg;Pwd=C!lantr3")
data <- sqlQuery(conn, "SELECT Top 1 * from DB_FB_CA.TechIndex_NPS_Daten")
close(conn)
conn
library(RODBC)
#conn <- odbcDriverConnect("Driver=Teradata;DBCName=DWH1;Uid=u4xk7mg;Pwd=C!lantr3")
conn <- odbcConnect(dsn="DWH1")
data <- sqlQuery(conn, "SELECT Top 1 * from DB_FB_CA.TechIndex_NPS_Daten")
close(conn)
library(RODBC)
conn <- odbcDriverConnect("Driver=Teradata;DBCName=DWH1;Uid=u4xk7mg;Pwd=C!lantr3")
#conn <- odbcConnect(dsn="DWH1")
data1 <- sqlQuery(conn, "SELECT Top 1 * from DB_FB_CA.TechIndex_NPS_Daten")
data2 <- RODBC::sqlQuery(conn, "sel * from DB_FB_CA.TechIndex_NPS_Daten") # took less than 2 mins to run
getwd()
save(data2, file = "~/testechindex.RData")
data3 <- data2
rm(data2)
load("~/testechindex.RData")
close(conn)
library(RODBC)
conn <- odbcDriverConnect("Driver=Teradata;DBCName=DWH1;Uid=u4xk7mg;Pwd=C!lantr3")
#conn <- odbcConnect(dsn="DWH1")
data1 <- sqlQuery(conn, "SELECT Top 1 * from DB_FB_CA.TechIndex_NPS_Daten")
conn <- odbcDriverConnect("Driver=Teradata;DBCName=DWH64;Uid=u4xk7mg;Pwd=C!lantr3")
close(conn)
conn
conn <- odbcDriverConnect("Driver=Teradata;DBCName=DWH64;Uid=u4xk7mg;Pwd=C!lantr3")
conn <- odbcDriverConnect("Driver=Teradata;DBCName=DWH64;Uid=u4xk7mg;Pwd=C!lantr3")
conn <- odbcConnect(dsn="DWH64")
conn
conn <- odbcDriverConnect("Driver=Teradata;DBCName=DWH64;Uid=u4xk7mg;Pwd=C!lantr3")
close(conn)
conn
conn <- odbcDriverConnect("Driver=Teradata;DBCName=DWH64;Uid=u4xk7mg;Pwd=C!lantr3")
library(explore)
input_pwd <- encrypt(getPass::getPass("password:"))
dwh <- dwh_connect("DWH64")
dwh
data <- dwh_read_data(dwh,"SELECT Top 1 * from DB_FB_CA.TechIndex_NPS_Daten")
data
dwh_disconnect(dwh)
dwh <- dwh_connect("DWH64")dwh <- dwh_connect("DWH1", user="u4xk7mg", pwd="C!lantr3")
dwh <- dwh_connect("DWH1", user="u4xk7mg", pwd="C!lantr3")
dwh
#data <- sqlQuery(conn, "SELECT Top 1 * from DB_FB_CA.TechIndex_NPS_Daten")
data <- dwh_read_data(dwh,"SELECT Top 1 * from DB_FB_CA.TechIndex_NPS_Daten")
dwh <- dwh_connect("DWH64", user="u4xk7mg", pwd="C!lantr3")
#data <- sqlQuery(conn, "SELECT Top 1 * from DB_FB_CA.TechIndex_NPS_Daten")
data <- dwh_read_data(dwh,"SELECT Top 1 * from DB_FB_CA.TechIndex_NPS_Daten")
rm(data)
rm(data())
rm(data1
)
library(sparklyr)
config <- spark_config()
config$spark.sql.hive.convertMetastoreParquet <- 'false' # When reading from and writing to Hive metastore Parquet tables, Spark SQL will try to use its own Parquet support instead of Hive SerDe for better performance. This behavior is controlled by the spark.sql.hive.convertMetastoreParquet configuration, and is turned on by default.
config$spark.driver.memory <- '20g'
sc <- spark_connect(master = "yarn-client", spark_home = "/usr/hdp/current/spark2-client", version = "2.2", config = config)
library(sparklyr)
library(readr)
library(dplyr)
setwd("~/cbb_app_split")
sc <- spark_connect(master = "local")
coltypes <- "iiiiiccccccccccicccccccciiiicccccciiciciTTcciiii"
colnames <- c('starttimesecond', 'starttimemilsec', 'endtimesecond', 'endtimemilsec', 'associationflag', 'imsi', 'msisdn', 'imeisv', 'ms_ip', 'server_ip', 'ms_port', 'server_port', 'ip_protocol', 'apn', 'charging_characteristics', 'rat_type', 'serving_node_ip', 'gateway_node_ip', 'cgi', 'sai', 'rai', 'tai', 'ecgi', 'lai', 'uplink_traffic', 'downlink_traffic', 'uplink_packets', 'downlink_packets', 'protocol_category', 'application', 'sub_application', 'egn_sub_protocol', 'url', 'user_agent', 'status_code', 'response_content_length', 'response_content_type', 'request_content_length', 'request_content_type', 'messageformatversion', 'starttime', 'endtime', 'mcc_location', 'mnc_location', 'year', 'month', 'day', 'hour')
csvfull <- read_csv(paste0('gamer1',".csv"),
col_names = colnames,
col_types = coltypes)
link <- copy_to(sc, csvfull, "df", overwrite = TRUE)
df_full <- csvfull %>%
mutate(ym = substr(starttime,1,7),
ymd = substring(starttime,1,10),
ymdh = substr(starttime,1,13),
ymdhm = substr(starttime,1,16),
traffic = uplink_traffic+downlink_traffic)
link_full <- link %>%
mutate(ym = substr(starttime,1,7),
ymd = substring(starttime,1,10),
ymdh = substr(starttime,1,13),
ymdhm = substr(starttime,1,16),
traffic = uplink_traffic+downlink_traffic)
df_full_ymdh <- df_full %>% select(ymdh) %>% collect()
df_full_ymd <- df_full %>% select(ymd) %>% collect()
link_full_ymdh <- link_full %>% select(ymdh) %>% collect()
link_full_ymd <- link_full %>% select(ymd) %>% collect()
nocoll <- df_full %>% select(ymdh)
mean(df_full_ymd == nocoll)
mean(df_full_ymdh == nocoll)
df_full_ymd <- df_full %>% select(ymd) %>% collect()
link_full_ymdh <- link_full %>% select(ymdh) %>% collect()
link_full_ymd <- link_full %>% select(ymd) %>% collect()
unique(df_full_ymd)
unique(link_full_ymd)
unique(df_full_ymdh)
unique(link_full_ymdh)
shiny::runApp(launch.browser = TRUE)
shiny::runApp(launch.browser = TRUE)
shiny::runApp(launch.browser = TRUE)
shiny::runApp(launch.browser = TRUE)
df <- read.parquet("gamer_one")
df2 <- df %>%
SparkR::filter(df$StartTime > '2018-12-13 00:00:00') %>%
SparkR::filter(df$StartTime < '2018-12-14 00:00:00')
df2 <- df %>%
SparkR::filter(df$StartTime > '2018-12-12 00:00:00') %>%
SparkR::filter(df$StartTime < '2018-12-12 23:59:59')
df3 <- df2 %>% # for the initial SparkDF
SparkR::mutate(ym = SparkR::substr(df2$StartTime, 1,7)) %>%
SparkR::mutate(ymd = SparkR::substr(df2$StartTime, 1,10)) %>%
SparkR::mutate(ymdh = SparkR::substr(df2$StartTime, 1,13)) %>%
SparkR::mutate(ymdhm = SparkR::substr(df2$StartTime, 1,16)) %>%
SparkR::mutate(traffic = df$uplink_traffic + df$downlink_traffic)
df4 <-   df3 %>% SparkR::groupBy('ymdh','protocol_category') %>%
# the sum function is why I need to create df2
SparkR::summarize(Traffic = sum(df3$traffic)) %>%
SparkR::arrange('Traffic', decreasing = TRUE) %>%
SparkR::collect()
df3 %>%
SparkR::filter(df3$StartTime <= 23:30) %>%
head()
df3 %>%
SparkR::filter(df3$StartTime <= '2018-12-12 23:30:00') %>%
head()
df3 %>%
SparkR::select(df3$StartTime) %>%
# SparkR::filter(df3$StartTime <= '2018-12-12 23:30:00') %>%
head()
x <- df3 %>%
SparkR::select(df3$StartTime) %>%
# SparkR::filter(df3$StartTime <= '2018-12-12 23:30:00') %>%
collect() %>% tibble()
x <- df3 %>%
SparkR::select(df3$StartTime) %>%
# SparkR::filter(df3$StartTime <= '2018-12-12 23:30:00') %>%
SparkR::collect() %>% tibble()
View(x)
View(x)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
df3 %>%
SparkR::select(df3$StartTime) %>%
SparkR::mutate(StartTime = SparkR::cast(df$StartTime, 'string')) %>%
head()
x <- df3 %>%
SparkR::select(df3$StartTime) %>%
SparkR::mutate(StartTime = SparkR::cast(df$StartTime, 'string')) %>%
# SparkR::filter(df3$StartTime <= '2018-12-12 23:30:00') %>%
SparkR::collect() %>% tibble()
View(x)
names(x)
x <- df3 %>%
SparkR::select(df3$StartTime) %>%
# SparkR::filter(df3$StartTime <= '2018-12-12 23:30:00') %>%
SparkR::collect() %>% tibble()
View(x)
df$StartTime
printSchema(df)
df$StartTime %>% head()
df %>% select(df$StartTime) %>% head()
df %>% SparkR::select(df$StartTime) %>% head()
df2 %>% SparkR::select(df2$StartTime) %>% head()
Sys.timezone()
from_utc_timestamp(2018-03-13T06:18:23+00:00)
from_utc_timestamp('2018-03-13T06:18:23+00:00')
from_utc_timestamp(df2$StartTime, "Europe/Berlin")
from_utc_timestamp(df2$StartTime, "Europe/Berlin") %>% head()
from_utc_timestamp(df2$StartTime, "Europe/Berlin")
to_utc_timestamp(df2$StartTime, "Europe/Berlin") %>% head()
df3 %>% select(df3$StartTime, df3$ymdh)
df3 %>% SparkR::select(df3$StartTime, df3$ymdh)
df3 %>% SparkR::select(df3$StartTime, df3$ymdh) %>% head(10)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
df3 %>% SparkR::select(df3$StartTime, df3$ymdh) %>% head(10)
df3 %>% SparkR::select(df3$StartTime, df3$ymdh)
df3 %>% SparkR::select(df3$StartTime, df3$ymdh) %>% head(10) %>% SparkR::collect()
df3 %>% SparkR::select(df3$StartTime, df3$ymdh) %>% head(10)
Sys.time()
Sys.date()
Sys.Date()
Sys.timezone()
Sys.timezone()
runApp(launch.browser = TRUE)
Sys.timezone()
Sys.timezone()
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
df3 %>% SparkR::select(df3$StartTime, df3$ymdh) %>% head(10)
df3 %>% SparkR::select(df3$StartTime, df3$ymdhm) %>% head(10)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
df2 %>% select(df2$IMSI) %>% head(1)
df2 %>% SparkR::select(df2$IMSI) %>% head(1)
df2 %>% SparkR::select(df2$IMSI) %>% pull(1)
df2 %>% SparkR::select(df2$IMSI) %>% head(1) %>% pull()
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
df2 <- df %>%
SparkR::filter(df$StartTime > '2018-11-05 00:00:00') %>%
SparkR::filter(df$StartTime < '2018-12-12 23:59:59')
df3 <- df2 %>% # for the initial SparkDF
SparkR::mutate(ym = SparkR::substr(df2$StartTime, 1,7)) %>%
SparkR::mutate(ymd = SparkR::substr(df2$StartTime, 1,10)) %>%
SparkR::mutate(ymdh = SparkR::substr(df2$StartTime, 1,13)) %>%
SparkR::mutate(ymdhm = SparkR::substr(df2$StartTime, 1,16)) %>%
SparkR::mutate(traffic = df$uplink_traffic + df$downlink_traffic)
df4 <-   df3 %>% SparkR::groupBy('ymdh','protocol_category') %>%
# the sum function is why I need to create df3
SparkR::summarize(Traffic = sum(df3$traffic)) %>%
SparkR::arrange('Traffic', decreasing = TRUE) %>%
SparkR::collect()
df4 <-   df3 %>% SparkR::groupBy('ymd','protocol_category') %>%
# the sum function is why I need to create df3
SparkR::summarize(Traffic = sum(df3$traffic)) %>%
SparkR::arrange('Traffic', decreasing = TRUE) %>%
SparkR::collect()
df4 <-   df3 %>% SparkR::groupBy('ymd','protocol_category') %>%
# the sum function is why I need to create df3
SparkR::summarize(Traffic = sum(df3$traffic)) %>%
SparkR::arrange('Traffic', decreasing = TRUE) %>%
SparkR::collect()
df3 <- df2 %>% # for the initial SparkDF
SparkR::mutate(ym = SparkR::substr(df2$StartTime, 1,7)) %>%
SparkR::mutate(ymd = SparkR::substr(df2$StartTime, 1,10)) %>%
SparkR::mutate(ymdh = SparkR::substr(df2$StartTime, 1,13)) %>%
SparkR::mutate(ymdhm = SparkR::substr(df2$StartTime, 1,16)) %>%
SparkR::mutate(traffic = df$uplink_traffic + df$downlink_traffic)
df2 <- df %>%
SparkR::filter(df$StartTime > '2018-11-05 00:00:00') %>%
SparkR::filter(df$StartTime < '2018-12-12 23:59:59')
sparkR.session.stop()
sparkR.session.stop()
spark <- sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
dataset <- "gamer1"
path <- paste0(dataset, ".csv")
dfcsv <- read.df(path, 'csv')
df <- read.parquet("gamer_one")
df2 <- df %>%
SparkR::filter(df$StartTime > '2018-11-05 00:00:00') %>%
SparkR::filter(df$StartTime < '2018-12-12 23:59:59')
df <- read.parquet("gamer_one")
df <- read.parquet("gamer_ten")
# setting up a local spark environment
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/home/spark")
}
#library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
library(conflicted)
library(magrittr) # for pipe
library(SparkR)
library(lubridate)
spark <- sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
df <- read.parquet("gamer_ten")
df2 <- df %>%
SparkR::filter(df$StartTime > '2018-11-05 00:00:00') %>%
SparkR::filter(df$StartTime < '2018-12-12 23:59:59')
df3 <- df2 %>% # for the initial SparkDF
SparkR::mutate(ym = SparkR::substr(df2$StartTime, 1,7)) %>%
SparkR::mutate(ymd = SparkR::substr(df2$StartTime, 1,10)) %>%
SparkR::mutate(ymdh = SparkR::substr(df2$StartTime, 1,13)) %>%
SparkR::mutate(ymdhm = SparkR::substr(df2$StartTime, 1,16)) %>%
SparkR::mutate(traffic = df$uplink_traffic + df$downlink_traffic)
df4 <-   df3 %>% SparkR::groupBy('ymd','protocol_category') %>%
# the sum function is why I need to create df3
SparkR::summarize(Traffic = sum(df3$traffic)) %>%
SparkR::arrange('Traffic', decreasing = TRUE) %>%
SparkR::collect()
View(df4)
shiny::runApp(launch.browser = TRUE)
ymd("2018-11")
5
ymd(paste("2018-11","-01")
)
ymd(paste("2018-11","-01"))
ymd(paste("2018-11","-01")) %>% class()
ymd(paste("2018-11","-01")) %>% floor_date("1 month") %>% class()
ymd(paste("2018-11","-01")) %>% floor_date("1 month")
ymd(paste("2018-11","-01")) %>% floor_date("2 months")
ymd(paste("2018-11","-01")) %>% floor_date("3 months")
df3 %>% head()
df3 %>%
SparkR::groupBy('ym','protocol_category') %>%
SparkR::summarize(Traffic = sum(two$traffic)) %>%
SparkR::arrange('Traffic', decreasing = TRUE)
df3 %>%
SparkR::groupBy('ym','protocol_category') %>%
SparkR::summarize(Traffic = sum(df3$traffic)) %>%
SparkR::arrange('Traffic', decreasing = TRUE)
df3 %>%
SparkR::groupBy('ym','protocol_category') %>%
SparkR::summarize(Traffic = sum(df3$traffic)) %>%
SparkR::arrange('Traffic', decreasing = TRUE) %>%
SparkR::collect() %>%
dplyr::mutate( Monat = (ymd(ymd) %>% floor_date('1 month')) ) %>%
dplyr::select(Monat, protocol_category, Traffic)
x <- df3 %>%
SparkR::groupBy('ym','protocol_category') %>%
SparkR::summarize(Traffic = sum(df3$traffic)) %>%
SparkR::arrange('Traffic', decreasing = TRUE) %>%
SparkR::collect()
View(x)
x %>% dplyr::mutate( Monat = (ymd(paste0(ym,'-01'))
)
x %>% dplyr::mutate( Monat = (ymd(paste0(ym,'-01'))))
x <- df3 %>%
SparkR::groupBy('ym','protocol_category') %>%
SparkR::summarize(Traffic = sum(df3$traffic)) %>%
SparkR::arrange('Traffic', decreasing = TRUE) %>%
SparkR::collect() %>%
dplyr::mutate( Monat = ymd(paste0(ym,'-01'))) %>%
dplyr::select(Monat, protocol_category, Traffic)
View(x)
str(x)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
shiny::runApp()
runApp()
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
getwd()
shiny::runApp()
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
spark_disconnect(sc)
library(SparkR)
spark_disconnect(sc)
sparkR.session.stop()
runApp(launch.browser = TRUE)
runApp()
runApp(launch.browser = TRUE)
runApp(launch.browser = TRUE)
library(readr)
gamerfull <- read_csv("gamerfull.csv", col_names = FALSE)
View(gamerfull)
object.size(gamerfull)
50*1024^2
3*1024^2
options(shiny.launch.browser = )
sc <- spark_connect(master = "local")
sc <- sparklr::spark_connect(master = "local")
sc <- sparklyr::spark_connect(master = "local")
getwd()
